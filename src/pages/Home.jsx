import React, { useState, useEffect } from 'react';
import { Link } from 'react-router-dom';
import { BookOpen, ArrowRight, Calendar, FolderOpen, List } from 'lucide-react';

// åˆ†ç»„æ•°æ®ç»“æ„
const paperGroups = [
  {
    id: 'my-work',
    title: 'My Work & Related æˆ‘çš„å·¥ä½œä¸ç›¸å…³è®ºæ–‡',
    papers: [
      {
        id: 'p-tuning-v2',
        title: 'P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks',
        authors: 'Xiao Liu, Kaixuan Ji, et al. (Tsinghua)',
        date: '2022-03',
        description: 'ã€Deep Prompt Tuningã€‘æ‰“ç ´ Prompt Tuning çš„å±€é™æ€§ï¼Œåœ¨æ¯ä¸€å±‚ Transformer åŠ å…¥å¯è®­ç»ƒ Prompt å‘é‡ã€‚ä»…å¾®è°ƒ 0.1%-3% å‚æ•°ï¼Œåœ¨ 330M-2B æ¨¡å‹è§„æ¨¡å’Œå›°éš¾åºåˆ—æ ‡æ³¨ä»»åŠ¡ï¼ˆNER/QAï¼‰ä¸Šè¾¾åˆ°ä¸ Fine-tuning ç›¸å½“çš„æ€§èƒ½ã€‚',
        tags: ['P-Tuning', 'Prompt Tuning', 'PEFT', 'NLU'],
        path: '/papers/p-tuning-v2'
      },
      {
        id: 'neftune',
        title: 'NEFTune: Noisy Embeddings Improve Instruction Finetuning',
        authors: 'Neel Jain et al.',
        date: '2023-10',
        description: 'ã€å™ªå£°åµŒå…¥æ­£åˆ™åŒ–ã€‘æç®€å´æƒŠäººçš„å¢å¼ºæŠ€æœ¯ï¼šä»…åœ¨è®­ç»ƒæ—¶çš„ Embedding ä¸Šæ·»åŠ éšæœºå™ªå£°ï¼ŒLLaMA-2-7B åœ¨ AlpacaEval ä» 29.8% é£™å‡è‡³ 64.7%ï¼ˆ+35%ï¼‰ã€‚é€šè¿‡ Î±/âˆš(Ld) ç¼©æ”¾å› å­æ§åˆ¶æ‰°åŠ¨å¼ºåº¦ï¼Œä½œä¸ºæ­£åˆ™åŒ–å™¨é˜²æ­¢è¿‡æ‹ŸåˆæŒ‡ä»¤æ ¼å¼ã€‚',
        tags: ['NEFTune', 'Regularization', 'Instruction Tuning', 'ICLR 2024'],
        path: '/papers/neftune'
      },
      {
        id: 'dcpc',
        title: 'Whose Instructions Count? Resolving Preference Bias in Instruction Fine-Tuning',
        authors: 'First Author Work',
        date: '2025-09',
        description: 'ã€ğŸ† NeurIPS 2025 ä¸€ä½œã€‘è§£å†³æŒ‡ä»¤å¾®è°ƒä¸­è¢«å¿½è§†çš„"åå¥½åå·®"é—®é¢˜ã€‚æå‡º DCPC (Dynamic Cross-Layer Preference Correction) æ¡†æ¶ï¼šé€šè¿‡åå¥½æ•æ„Ÿç›¸ä¼¼åº¦æ£€æµ‹å†²çªã€è·¨å±‚å‰ç¼€å¯¹é½æ‹‰è¿‘è¡¨ç¤ºã€PCM æ¨¡å—æ³¨å…¥å…±è¯†åå¥½ã€‚åœ¨åå¥½åç§»æ•°æ®é›†ä¸Š Acc æå‡ 4-6.7%ï¼Œæ–¹å·®é™ä½ 35%ã€‚',
        tags: ['DCPC', 'Preference Bias', 'IFT', 'NeurIPS 2025', 'ğŸ† First Author'],
        path: '/papers/dcpc'
      }
    ]
  },
  {
    id: 'deepseek-series',
    title: 'DeepSeek Series',
    papers: [
      {
        id: 'deepseek-v3',
        title: 'DeepSeek-V3 Technical Report',
        authors: 'DeepSeek-AI',
        date: '2024-12',
        description: 'A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.',
        tags: ['LLM', 'MoE', 'Transformer'],
        path: '/papers/deepseek-v3'
      },
      {
        id: 'deepseek-v2',
        title: 'DeepSeek-V2: A Strong, Economical, and Efficient MoE Language Model',
        authors: 'DeepSeek-AI',
        date: '2024-05',
        description: 'Introducing MLA (Multi-Head Latent Attention) and DeepSeekMoE. Strong performance with significantly reduced training costs and KV cache.',
        tags: ['LLM', 'MoE', 'MLA', 'GRPO'],
        path: '/papers/deepseek-v2'
      },
      {
        id: 'deepseek-llm',
        title: 'DeepSeek LLM: Scaling Open-Source Language Models with Longtermism',
        authors: 'DeepSeek-AI',
        date: '2024-01',
        description: 'æ·±å…¥æ¢ç´¢ DeepSeek LLM (7B & 67B) çš„æ‰©å±•å®šå¾‹ã€æ¶æ„åˆ›æ–°ä¸å“è¶Šæ€§èƒ½ã€‚åœ¨ä»£ç ã€æ•°å­¦å’Œæ¨ç†é¢†åŸŸè¶…è¶Š LLaMA-2 70Bã€‚',
        tags: ['LLM', 'Scaling Laws', 'Dense'],
        path: '/papers/deepseek-llm'
      },
      {
        id: 'deepseek-math-v2',
        title: 'DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning',
        authors: 'DeepSeek-AI',
        date: '2025-01',
        description: 'ä»â€œç›²ç›®åˆ·é¢˜â€åˆ°â€œè‡ªæˆ‘åæ€â€çš„èŒƒå¼è½¬å˜ã€‚å¼•å…¥è¿‡ç¨‹ç›‘ç£ä¸è‡ªéªŒè¯æœºåˆ¶ï¼Œé€šè¿‡ GRPO æå‡æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚',
        tags: ['Self-Verification', 'Process Supervision', 'RL'],
        path: '/papers/deepseek-math-v2'
      },
      {
        id: 'deepseek-prover-v2',
        title: 'DeepSeek-Prover-V2: Unlocking the Potential of LLMs for Formal Math',
        authors: 'DeepSeek-AI',
        date: '2024-06',
        description: 'æ¨è¿›å½¢å¼åŒ–æ•°å­¦æ¨ç†ï¼Œé‡‡ç”¨é€’å½’å­ç›®æ ‡åˆ†è§£å’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œåœ¨ MiniF2F æµ‹è¯•ä¸­åˆ·æ–° SOTAã€‚',
        tags: ['Formal Math', 'RL', 'Reasoning'],
        path: '/papers/deepseek-prover-v2'
      },
      {
        id: 'deepseek-v32',
        title: 'DeepSeek-V3.2: Pushing the Frontier of Open-Source LLMs',
        authors: 'DeepSeek-AI',
        date: '2025-06',
        description: 'ã€DSA + Scalable RLã€‘å¼•å…¥ DeepSeek Sparse Attention (DSA) é«˜æ•ˆç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡æ— å KL ä¼°è®¡å™¨ç¨³å®šå¤§è§„æ¨¡ GRPO è®­ç»ƒã€‚1800+ ç¯å¢ƒ + 85K Prompt çš„ Agent æ•°æ®åˆæˆæµæ°´çº¿ã€‚IMO/IOI 2025 é‡‘ç‰Œæ°´å¹³ã€‚',
        tags: ['DSA', 'Sparse Attention', 'GRPO', 'Agent'],
        path: '/papers/deepseek-v32'
      }
    ]
  },
  {
    id: 'qwen-series',
    title: 'Qwen Series',
    papers: [
      {
        id: 'qwen1',
        title: 'Qwen Technical Report',
        authors: 'Alibaba Cloud',
        date: '2023-09',
        description: 'The first comprehensive technical report of Qwen series (7B/14B). Detailed breakdown of architecture (RoPE, SwiGLU, RMSNorm) and alignment.',
        tags: ['LLM', 'Transformer', 'Alignment'],
        path: '/papers/qwen1' 
      },
      {
        id: 'qwen2',
        title: 'Qwen2 Technical Report',
        authors: 'Alibaba Cloud',
        date: '2024-07',
        description: 'æ–°ä¸€ä»£ Qwen ç³»åˆ—ï¼Œ0.5B-72B å…¨å°ºå¯¸è¦†ç›–ã€‚å¼•å…¥ MoE æ¶æ„ (57B-A14B)ï¼Œ7T é«˜è´¨é‡æ•°æ®é¢„è®­ç»ƒï¼Œæ”¯æŒ 128K é•¿æ–‡æœ¬ã€‚',
        tags: ['LLM', 'MoE', 'Long Context', 'Multilingual'],
        path: '/papers/qwen2'
      },
      {
        id: 'qwen25',
        title: 'Qwen2.5 Technical Report',
        authors: 'Alibaba Cloud',
        date: '2024-09',
        description: 'Qwen ç³»åˆ—æœ€æ–°è¿­ä»£ï¼Œé¢„è®­ç»ƒæ•°æ®ä» 7T æ‰©å±•è‡³ 18T tokensã€‚åŒé˜¶æ®µ RL ç­–ç•¥ï¼šOffline DPO + Online GRPOã€‚æ”¯æŒ 1M ä¸Šä¸‹æ–‡ (Turbo)ï¼Œç»“åˆ YaRN ä¸ DCA å®ç°é•¿åº¦å¤–æ¨ã€‚',
        tags: ['LLM', '18T Tokens', 'GRPO', '128K Context'],
        path: '/papers/qwen25'
      },
      {
        id: 'gated-attention',
        title: 'Gated Attention: Non-linearity, Sparsity, and Attention-Sink-Free',
        authors: 'Qwen Team',
        date: '2024-12',
        description: 'ã€é—¨æ§æ³¨æ„åŠ›ã€‘åœ¨ SDPA è¾“å‡ºåæ·»åŠ  Head-specific Sigmoid Gateï¼Œå¼•å…¥éçº¿æ€§æ‰“ç ´ä½ç§©ç“¶é¢ˆï¼Œé€šè¿‡è¾“å…¥ä¾èµ–ç¨€ç–æ€§æ¶ˆé™¤ Attention Sink ç°è±¡ï¼Œæ˜¾è‘—æå‡è®­ç»ƒç¨³å®šæ€§ã€‚',
        tags: ['Gated Attention', 'Attention Sink', 'Sparsity'],
        path: '/papers/gated-attention'
      },
      {
        id: 'qwen3',
        title: 'Qwen3 Technical Report',
        authors: 'Alibaba Cloud',
        date: '2025-04',
        description: 'Qwen3 ç³»åˆ—ï¼šç»Ÿä¸€æ€è€ƒæ¨¡å¼æ¡†æ¶ï¼Œé€šè¿‡ /think å’Œ /no_think æ ‡è®°åŠ¨æ€åˆ‡æ¢ã€‚36T tokens é¢„è®­ç»ƒï¼Œ119 ç§è¯­è¨€æ”¯æŒã€‚å››é˜¶æ®µåè®­ç»ƒ + å¼ºåˆ°å¼±è’¸é¦ã€‚æ——èˆ° 235B-A22B è¶…è¶Š DeepSeek-R1 å’Œ o1ã€‚',
        tags: ['LLM', 'MoE', 'Thinking Mode', '36T Tokens'],
        path: '/papers/qwen3'
      }
    ]
  },
  {
    id: 'llama-series',
    title: 'LLaMA Series',
    papers: [
      {
        id: 'llama-deep-dive',
        title: 'LLaMA: Open and Efficient Foundation Language Models',
        authors: 'Meta AI',
        date: '2023-02',
        description: 'æ‰“ç ´"å‚æ•°è‡³ä¸Š"çš„è¿·ä¿¡ï¼Œç”¨å…¬å¼€æ•°æ®å’Œæè‡´å·¥ç¨‹ä¼˜åŒ–é‡æ–°å®šä¹‰å¤§æ¨¡å‹è®­ç»ƒèŒƒå¼ã€‚13B æ¨¡å‹å‡»è´¥ GPT-3 (175B)ã€‚',
        tags: ['LLM', 'Transformer', 'Open Source'],
        path: '/papers/llama-deep-dive'
      },
      {
        id: 'llama2',
        title: 'Llama 2: Open Foundation and Fine-Tuned Chat Models',
        authors: 'Meta AI',
        date: '2023-07',
        description: 'ä»é¢„è®­ç»ƒåˆ° RLHF å¾®è°ƒçš„å…¨æµç¨‹æŠ«éœ²ï¼ŒåŒå¥–åŠ±æ¨¡å‹è§£å†³æœ‰ç”¨æ€§ä¸å®‰å…¨æ€§å†²çªï¼ŒGhost Attention è§£å†³å¤šè½®å¯¹è¯é—å¿˜é—®é¢˜ã€‚',
        tags: ['LLM', 'RLHF', 'Chat', 'Safety'],
        path: '/papers/llama2'
      },
      {
        id: 'llama3',
        title: 'The Llama 3 Herd of Models',
        authors: 'Meta AI',
        date: '2024-07',
        description: '405B ç¨ å¯†æ¶æ„æ——èˆ°æ¨¡å‹ï¼Œ15.6T Token è®­ç»ƒæ•°æ®ï¼Œ128K ä¸Šä¸‹æ–‡çª—å£ã€‚åšæŒç®€å•æ¶æ„ï¼Œé€šè¿‡æè‡´æ•°æ®å·¥ç¨‹å¯¹æ ‡ GPT-4ã€‚',
        tags: ['LLM', 'Multimodal', 'Scaling Laws', '405B'],
        path: '/papers/llama3'
      }
    ]
  },
  {
    id: 'post-training',
    title: 'Post-training åè®­ç»ƒ',
    papers: [
      {
        id: 'deep-rl-human-prefs',
        title: 'Deep Reinforcement Learning from Human Preferences',
        authors: 'OpenAI & DeepMind',
        date: '2017-06',
        description: 'ã€RLHF å¥ åŸºä¹‹ä½œã€‘æå‡ºé€šè¿‡äººç±»åå¥½æ¯”è¾ƒæ¥å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œç”¨æå°‘çš„äººç±»åé¦ˆï¼ˆ<1%ï¼‰å®Œæˆå¤æ‚ä»»åŠ¡ã€‚Bradley-Terry æ¨¡å‹ + äº¤å‰ç†µæŸå¤±ã€‚',
        tags: ['RLHF', 'Reward Model', 'Human Feedback'],
        path: '/papers/deep-rl-human-prefs'
      },
      {
        id: 'instructgpt',
        title: 'InstructGPT: Training LMs to Follow Instructions with Human Feedback',
        authors: 'OpenAI (Ouyang et al.)',
        date: '2022-03',
        description: 'ã€RLHF å·¥ç¨‹å®è·µã€‘å®Œæ•´çš„ RLHF ä¸‰æ­¥æ³•ï¼šSFT â†’ å¥–åŠ±æ¨¡å‹ â†’ PPOã€‚1.3B InstructGPT åœ¨äººç±»è¯„ä¼°ä¸­èƒœè¿‡ 175B GPT-3ã€‚',
        tags: ['RLHF', 'PPO', 'SFT', 'Alignment'],
        path: '/papers/instructgpt'
      },
      {
        id: 'deepseek-math',
        title: 'DeepSeekMath: Pushing the Limits of Mathematical Reasoning',
        authors: 'DeepSeek-AI',
        date: '2024-02',
        description: 'ã€GRPO å¼ºåŒ–å­¦ä¹ ã€‘æå‡º Group Relative Policy Optimizationï¼Œæ— éœ€ Critic Modelã€‚7B æ¨¡å‹åœ¨ MATH è¾¾åˆ° 51.7%ï¼Œæ¥è¿‘ GPT-4ã€‚',
        tags: ['GRPO', 'Math Reasoning', 'RL'],
        path: '/papers/deepseek-math'
      },
      {
        id: 'gspo',
        title: 'Group Sequence Policy Optimization (GSPO)',
        authors: 'Qwen Team (Alibaba)',
        date: '2025-07',
        description: 'ã€Qwen3 èƒŒåçš„ RL åŸºçŸ³ã€‘åºåˆ—çº§é‡è¦æ€§æ¯”ç‡æ›¿ä»£ Token çº§ï¼Œå½»åº•è§£å†³é•¿æ–‡æœ¬è®­ç»ƒä¸­çš„æ¨¡å‹åå¡Œé—®é¢˜ã€‚å®Œç¾é€‚é… MoE æ¶æ„ã€‚',
        tags: ['GSPO', 'Sequence-level RL', 'MoE'],
        path: '/papers/gspo'
      },
      {
        id: 'aspo',
        title: 'ASPO: Asymmetric Importance Sampling Policy Optimization',
        authors: 'Kuaishou & Tsinghua',
        date: '2025-10',
        description: 'ã€å¿«æ‰‹ ASPO ç®—æ³•ã€‘æ­ç¤º GRPO æ­£æ ·æœ¬æƒé‡é”™é…é—®é¢˜ï¼Œé€šè¿‡åè½¬æ­£ä¼˜åŠ¿ Token çš„ IS æ¯”ç‡ + è½¯åŒé‡æˆªæ–­ï¼Œæ˜¾è‘—æå‡è®­ç»ƒç¨³å®šæ€§å’Œæ¨ç†æ€§èƒ½ã€‚',
        tags: ['ASPO', 'IS Ratio', 'OSRL'],
        path: '/papers/aspo'
      },
      {
        id: 'dpo',
        title: 'DPO: Direct Preference Optimization',
        authors: 'Stanford (Rafailov et al.)',
        date: '2023-05',
        description: 'ã€Stanford DPO ç®—æ³•ã€‘æ­ç¤ºè¯­è¨€æ¨¡å‹ä¸å¥–åŠ±æ¨¡å‹çš„å¯¹å¶æ€§ï¼Œé€šè¿‡æœ€ä¼˜ç­–ç•¥è§£æè§£é‡å‚æ•°åŒ–å¥–åŠ±å‡½æ•°ï¼Œå°†å¤æ‚ RLHF è½¬åŒ–ä¸ºç®€å•äºŒåˆ†ç±»æŸå¤±ï¼Œæ— éœ€æ˜¾å¼å¥–åŠ±å»ºæ¨¡å³å¯å®ç°ç¨³å®šé«˜æ•ˆçš„åå¥½å¯¹é½ã€‚',
        tags: ['DPO', 'Preference Learning', 'No RL'],
        path: '/papers/dpo'
      },
      {
        id: 'lora',
        title: 'LoRA: Low-Rank Adaptation of Large Language Models',
        authors: 'Microsoft',
        date: '2021-06',
        description: 'ã€å¾®è½¯ LoRA ç®—æ³•ã€‘æ­ç¤ºå¤§æ¨¡å‹é€‚åº”ä»»åŠ¡æ—¶çš„æƒé‡æ›´æ–°å…·æœ‰æä½"å†…åœ¨ç§©"çš„ç‰¹æ€§ï¼Œé€šè¿‡å†»ç»“é¢„è®­ç»ƒæƒé‡å¹¶æ³¨å…¥å¹¶è¡Œçš„ä½ç§©åˆ†è§£çŸ©é˜µï¼Œæ˜¾è‘—å‡å°‘ä¸‡å€è®­ç»ƒå‚æ•°å¹¶æ¶ˆé™¤æ¨ç†å»¶è¿Ÿï¼Œæ€§èƒ½åŒ¹æ•Œå…¨é‡å¾®è°ƒã€‚',
        tags: ['LoRA', 'PEFT', 'Low-Rank'],
        path: '/papers/lora'
      },
      {
        id: 'lora-plus',
        title: 'LoRA+: Efficient Low Rank Adaptation of Large Models',
        authors: 'UC Berkeley & ICML 2024',
        date: '2024-02',
        description: 'ã€LoRA æ”¹è¿›ç‰ˆã€‘é€šè¿‡é‡çº²åˆ†ææ­ç¤ºæ ‡å‡† LoRA åœ¨å¤§å®½åº¦æ¨¡å‹ä¸Šçš„ä½æ•ˆæœ¬è´¨ï¼šA å’Œ B çŸ©é˜µå¯¹è¾“å…¥ç»´åº¦ n çš„ä¾èµ–ä¸åŒã€‚æ ¸å¿ƒä¿®æ­£ï¼šä¸º B è®¾ç½®æ›´é«˜å­¦ä¹ ç‡ï¼ˆÎ»â‰ˆ16ï¼‰ï¼Œè®­ç»ƒé€Ÿåº¦æå‡ 2 å€ã€‚',
        tags: ['LoRA+', 'Learning Rate', 'PEFT'],
        path: '/papers/lora-plus'
      },
      {
        id: 'lora-fa',
        title: 'LoRA-FA: Memory-Efficient Low-Rank Adaptation',
        authors: 'HKBU & HKUST',
        date: '2023-08',
        description: 'ã€æ˜¾å­˜ä¼˜åŒ– LoRAã€‘å†»ç»“æŠ•å½±é™ç»´çŸ©é˜µ Aï¼Œä»…æ›´æ–°æŠ•å½±å‡ç»´çŸ©é˜µ Bã€‚æ ¸å¿ƒæ´å¯Ÿï¼šè®¡ç®— A çš„æ¢¯åº¦éœ€å­˜å‚¨é«˜ç»´è¾“å…¥ Xï¼Œè€Œè®¡ç®— B çš„æ¢¯åº¦åªéœ€ä½ç»´ XAï¼Œæ˜¾å­˜é™ä½æ•°ç™¾å€ã€‚',
        tags: ['LoRA-FA', 'Memory Efficient', 'Frozen-A'],
        path: '/papers/lora-fa'
      },
      {
        id: 'adalora',
        title: 'AdaLoRA: Adaptive Budget Allocation for PEFT',
        authors: 'ICLR 2023',
        date: '2023-03',
        description: 'ã€è‡ªé€‚åº” LoRAã€‘ä»¥ SVD å½¢å¼å‚æ•°åŒ–å¢é‡æ›´æ–° (PÂ·Î›Â·Q)ï¼Œæ ¹æ®é‡è¦æ€§å¾—åˆ†åŠ¨æ€åˆ†é…å‚æ•°é¢„ç®—ã€‚æ ¸å¿ƒå‘ç°ï¼šFFN å’Œé«˜å±‚ç½‘ç»œéœ€è¦æ›´å¤šçš„ç§©ï¼Œé€šè¿‡ç«‹æ–¹è¡°å‡ç­–ç•¥é€æ­¥å‰ªæã€‚',
        tags: ['AdaLoRA', 'SVD', 'Rank Allocation'],
        path: '/papers/adalora'
      },
      {
        id: 'dora',
        title: 'DoRA: Weight-Decomposed Low-Rank Adaptation',
        authors: 'NVIDIA & ICML 2024',
        date: '2024-02',
        description: 'ã€æƒé‡åˆ†è§£ LoRAã€‘å°†æƒé‡åˆ†è§£ä¸ºå¹…åº¦ m å’Œæ–¹å‘ Vï¼Œæ­ç¤º LoRA ä¸ FT å­¦ä¹ æ¨¡å¼å·®å¼‚ï¼šLoRA è€¦åˆå¹…åº¦/æ–¹å‘ï¼ˆæ­£ç›¸å…³ï¼‰ï¼ŒFT è§£è€¦ï¼ˆè´Ÿç›¸å…³ï¼‰ã€‚DoRA æ¨¡æ‹Ÿ FTï¼Œæ€§èƒ½æå‡ 2-3%ã€‚',
        tags: ['DoRA', 'Weight Decomposition', 'Magnitude-Direction'],
        path: '/papers/dora'
      }
    ]
  },
  {
    id: 'multimodal',
    title: 'Multimodal å¤šæ¨¡æ€ç»¼è¿°',
    papers: [
      {
        id: 'mllm-survey',
        title: 'A Survey on Multimodal Large Language Models',
        authors: 'arXiv 2023',
        date: '2023-06',
        description: 'ã€MLLM ç»¼åˆç»¼è¿°ã€‘ç³»ç»Ÿæ¢³ç†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼šä¸‰å¤§æ ¸å¿ƒæ¨¡å—ï¼ˆæ¨¡æ€ç¼–ç å™¨ + LLM + æ¨¡æ€æ¥å£ï¼‰ã€ä¸‰é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼ˆé¢„è®­ç»ƒ â†’ æŒ‡ä»¤å¾®è°ƒ â†’ å¯¹é½ï¼‰ã€å¤šæ¨¡æ€å¹»è§‰é—®é¢˜ä¸ç¼“è§£æ–¹æ³•ã€M-ICL/M-CoT æ‰©å±•æŠ€æœ¯ã€‚',
        tags: ['Survey', 'MLLM', 'Multimodal', 'Vision-Language'],
        path: '/papers/mllm-survey'
      },
      {
        id: 'mllm-overview',
        title: 'Multimodal Large Language Models: A Survey',
        authors: 'Jiayang Wu et al.',
        date: '2023-11',
        description: 'ã€MLLM å‘å±•å²ç»¼è¿°ã€‘ä»å†å²è„‰ç»œåˆ°æŠ€æœ¯å…¨æ™¯ï¼šå››å¤§é˜¶æ®µå‘å±•å†ç¨‹ï¼ˆå•æ¨¡æ€â†’æ¨¡æ€è½¬æ¢â†’æ¨¡æ€èåˆâ†’å¤§è§„æ¨¡ MLLMï¼‰ã€ViT/CLIP/BLIP-2/LLaVA ç­‰æ ¸å¿ƒæ¨¡å‹è§£æã€ITC/MLM ç­‰è®­ç»ƒç›®æ ‡å…¬å¼æ¨å¯¼ã€AGI æŒ‘æˆ˜ä¸ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚',
        tags: ['Survey', 'MLLM', 'CLIP', 'LLaVA'],
        path: '/papers/mllm-overview'
      },
      {
        id: 'vit-survey',
        title: 'Visual Instruction Tuning towards General-Purpose Multimodal Model',
        authors: 'Survey 2023',
        date: '2023',
        description: 'ã€è§†è§‰æŒ‡ä»¤å¾®è°ƒç»¼è¿°ã€‘ä»ä¼ ç»Ÿä»»åŠ¡ç‰¹å®šèŒƒå¼åˆ°åŸºäºæŒ‡ä»¤çš„é€šç”¨èŒƒå¼è½¬å˜ã€‚ç³»ç»Ÿæ¢³ç† GPMM ä¸‰å¤§æ”¯æŸ±ï¼šæ•°æ®æ„å»ºï¼ˆGPT-4 æ‰©å±•ï¼‰ã€æ¶æ„è®¾è®¡ï¼ˆVision Encoder + Adapter + LLMï¼‰ã€ä¸¤é˜¶æ®µè®­ç»ƒã€‚æ¶µç›–åˆ¤åˆ«/ç”Ÿæˆ/æ¨ç†ä»»åŠ¡åŠè§†é¢‘/3D/åŒ»ç–—é¢†åŸŸåº”ç”¨ã€‚',
        tags: ['Survey', 'VIT', 'LLaVA', 'LISA'],
        path: '/papers/vit-survey'
      },
      {
        id: 'mm-llms',
        title: 'MM-LLMs: Recent Advances in MultiModal Large Language Models',
        authors: 'Survey 2024',
        date: '2024',
        description: 'ã€MM-LLMs æœ€æ–°è¿›å±•ã€‘ç³»ç»Ÿåˆ†æ 126 ä¸ªå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼šäº”å¤§æ ¸å¿ƒç»„ä»¶ï¼ˆæ¨¡æ€ç¼–ç å™¨ + è¾“å…¥æŠ•å½±å™¨ + LLM + è¾“å‡ºæŠ•å½±å™¨ + æ¨¡æ€ç”Ÿæˆå™¨ï¼‰ã€LDM ç”ŸæˆåŸç†ã€ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼ˆMM PT + MM ITï¼‰ã€‚æ·±åº¦è§£æ Any-to-Any æ¶æ„ä¸æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶æ§åˆ¶æœºåˆ¶ã€‚',
        tags: ['Survey', 'MM-LLMs', 'LDM', 'Any-to-Any'],
        path: '/papers/mm-llms'
      },
      {
        id: 'mllm-revolution',
        title: 'The Revolution of Multimodal Large Language Models: A Survey',
        authors: 'arXiv 2024',
        date: '2024-02',
        description: 'ã€MLLM é©å‘½ç»¼è¿°ã€‘ç³»ç»Ÿè§£æè§†è§‰ MLLM çš„æ¶æ„é€‰æ‹©ï¼ˆViT + LLM + Adapterï¼‰ã€ä¸‰å¤§æ ¸å¿ƒå…¬å¼ï¼ˆCross-Attention/è‡ªå›å½’æŸå¤±/ITC å¯¹æ¯”å­¦ä¹ ï¼‰ã€ä¸¤é˜¶æ®µè®­ç»ƒï¼ˆç‰¹å¾å¯¹é½ + æŒ‡ä»¤å¾®è°ƒï¼‰ã€‚æ¶µç›–è§†è§‰å®šä½ã€å›¾åƒç”Ÿæˆã€è§†é¢‘ç†è§£åŠåŒ»ç–—/è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸåº”ç”¨ã€‚',
        tags: ['Survey', 'MLLM', 'Cross-Attention', 'ITC'],
        path: '/papers/mllm-revolution'
      },
      {
        id: 'efficient-mllms',
        title: 'Efficient Multimodal Large Language Models: A Survey',
        authors: 'arXiv 2024',
        date: '2024-05',
        description: 'ã€é«˜æ•ˆ MLLM ç»¼è¿°ã€‘æ·±åº¦è§£æè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ï¼šè½»é‡åŒ–æ¶æ„ï¼ˆMobileVLM/TinyLLaVAï¼‰ã€è§†è§‰ Token å‹ç¼©ï¼ˆLLaVA-UHD/Mini-Geminiï¼‰ã€é«˜æ•ˆç»“æ„ï¼ˆMoE/Mamba/SSMï¼‰ã€PEFT æŠ€æœ¯ï¼ˆLoRA-FA/LOMOï¼‰ã€‚æ¶µç›–å‰ªæã€è’¸é¦ã€é‡åŒ–åŠ ShareGPT4V æ•°æ®ç”ŸæˆèŒƒå¼ã€‚',
        tags: ['Survey', 'Efficient', 'MoE', 'Mamba', 'PEFT'],
        path: '/papers/efficient-mllms'
      },
      {
        id: 'mllm-comprehensive',
        title: 'A Comprehensive Review of Multimodal Large Language Models',
        authors: 'arXiv 2024',
        date: '2024-08',
        description: 'ã€MLLM å…¨é¢ç»¼è¿°ã€‘ä» LLaVA åˆ° Soraï¼Œç³»ç»Ÿè§£æå›¾åƒ/è§†é¢‘/éŸ³é¢‘ä¸‰å¤§æ¨¡æ€ï¼šè¾“å…¥ç¼–ç å™¨ï¼ˆViT/Whisper/ImageBindï¼‰ã€ç‰¹å¾èåˆï¼ˆLinear/Q-Former/Cross-Attentionï¼‰ã€ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ã€‚æ¶µç›– Video-LLaMAã€Qwen-Audioã€SpeechGPT ç­‰ä»£è¡¨æ¨¡å‹åŠå¹»è§‰/è®¡ç®—æˆæœ¬ç­‰å…³é”®æŒ‘æˆ˜ã€‚',
        tags: ['Survey', 'MLLM', 'Video', 'Audio', 'Sora'],
        path: '/papers/mllm-comprehensive'
      },
      {
        id: 'mech-interp-survey',
        title: 'A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models',
        authors: 'arXiv 2025',
        date: '2025-02',
        description: 'ã€æœºæ¢°å¯è§£é‡Šæ€§ç»¼è¿°ã€‘æ·±åº¦è§£æ MLLM å†…éƒ¨æœºåˆ¶ï¼šä¸‰ç»´åˆ†ç±»ä½“ç³»ï¼ˆæ¨¡å‹å®¶æ—/è§£é‡ŠæŠ€æœ¯/ä¸‹æ¸¸åº”ç”¨ï¼‰ã€LLM é€‚é…æ–¹æ³•ï¼ˆçº¿æ€§æ¢æµ‹/Logit Lens/å› æœè¿½è¸ª/SAEï¼‰ã€å¤šæ¨¡æ€ç‰¹æœ‰æŠ€æœ¯ï¼ˆCross-Attention åˆ†æ/TextSpan/ç½‘ç»œè§£å‰–ï¼‰ã€‚æ¶µç›–æ¨¡å‹ç¼–è¾‘ã€å¹»è§‰æ£€æµ‹ã€å®‰å…¨å»åç­‰å…³é”®åº”ç”¨ã€‚',
        tags: ['Survey', 'Interpretability', 'CLIP', 'Diffusion', 'SAE'],
        path: '/papers/mech-interp-survey'
      },
      {
        id: 'understanding-mllms',
        title: 'Understanding Multimodal LLMs',
        authors: 'Sebastian Raschka',
        date: '2024',
        description: 'ã€å¤šæ¨¡æ€ LLM æ¶æ„æŒ‡å—ã€‘æ·±åº¦è§£æä¸¤å¤§æ ¸å¿ƒèŒƒå¼ï¼šç»Ÿä¸€åµŒå…¥æ¶æ„ï¼ˆLLaVA/Molmo/Qwen2-VLï¼‰vs äº¤å‰æ³¨æ„åŠ›æ¶æ„ï¼ˆLlama 3.2/NVLM-X/Flamingoï¼‰ã€‚æ¶µç›– Vision Encoderï¼ˆViT/CLIP/SigLIPï¼‰ã€Projectorï¼ˆLinear/MLP/Perceiverï¼‰æŠ€æœ¯ç»†èŠ‚ï¼Œå¹¶æ·±åº¦ç›˜ç‚¹ 2024 å¹´åå¤§å‰æ²¿æ¨¡å‹åŠ NVLM å¯¹æ¯”ç»“è®ºã€‚',
        tags: ['Guide', 'Architecture', 'LLaVA', 'NVLM', 'Llama 3.2'],
        path: '/papers/understanding-mllms'
      },
      {
        id: 'vlm-knowledge-base',
        title: 'VLM Knowledge Base: è§†è§‰è¯­è¨€æ¨¡å‹å®Œå…¨æŒ‡å—',
        authors: 'Multi-source Summary',
        date: '2024',
        description: 'ã€VLM æŠ€æœ¯ç™¾ç§‘ã€‘ç³»ç»Ÿæ•´ç† 15+ ç»å…¸ä¸å‰æ²¿è§†è§‰è¯­è¨€æ¨¡å‹ã€‚åŸºç¡€æ¶æ„ï¼ˆFlamingo/BLIP-2ï¼‰ã€æŒ‡ä»¤å¾®è°ƒï¼ˆInstructBLIP/LLaVA/MiniGPT-4ï¼‰ã€æ¶æ„åˆ›æ–°ï¼ˆCogVLM/Qwen-VL/Yi-VLï¼‰ã€é«˜åˆ†è¾¨ç‡æ–¹æ¡ˆï¼ˆMonkey/LLaVA-UHD/MoE-LLaVAï¼‰ã€å…¨æ¨¡æ€ä¸è§†é¢‘ï¼ˆNExT-GPT/LLaMA-VIDï¼‰ã€‚æ¶µç›– Q-Formerã€Perceiver Resamplerã€Visual Expertã€AnyRes ç­‰æ ¸å¿ƒæŠ€æœ¯ã€‚',
        tags: ['Knowledge Base', 'Flamingo', 'BLIP-2', 'LLaVA', 'CogVLM'],
        path: '/papers/vlm-knowledge-base'
      }
    ]
  },
  {
    id: 'vision-encoder',
    title: 'Vision Encoder å›¾åƒç¼–ç å™¨',
    papers: [
      {
        id: 'vision-transformer',
        title: 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale',
        authors: 'Alexey Dosovitskiy et al.',
        date: 'ICLR 2021',
        description: 'ã€ViT å¼€å±±ä¹‹ä½œã€‘æå‡ºçº¯ Transformer å›¾åƒåˆ†ç±»æ¶æ„ã€‚æ ¸å¿ƒæ€æƒ³ï¼šå°†å›¾åƒåˆ‡åˆ†ä¸º 16Ã—16 Patch åºåˆ—ï¼Œé€šè¿‡çº¿æ€§æŠ•å½± + ä½ç½®ç¼–ç  + [CLS] Token è¾“å…¥æ ‡å‡† Transformer ç¼–ç å™¨ã€‚æ·±åº¦è§£æå››å¤§æ ¸å¿ƒå…¬å¼ï¼ˆåµŒå…¥å±‚/ç¼–ç å™¨/åˆ†ç±»å¤´/Attentionï¼‰ï¼Œå¯¹æ¯” CNN å½’çº³åç½®å·®å¼‚ã€‚JFT-300M é¢„è®­ç»ƒåè¾¾åˆ° 88.55% ImageNet Top-1ï¼Œè®¡ç®—é‡ä»…ä¸º ResNet çš„ 1/4ã€‚',
        tags: ['ViT', 'Transformer', 'Patch Embedding', 'ICLR 2021'],
        path: '/papers/vision-transformer'
      },
      {
        id: 'clip',
        title: 'CLIP: Learning Transferable Visual Models From Natural Language Supervision',
        authors: 'OpenAI (Radford et al.)',
        date: '2021-02',
        description: 'ã€å¯¹æ¯”å­¦ä¹ é‡Œç¨‹ç¢‘ã€‘é€šè¿‡å¯¹æ¯”å­¦ä¹ åœ¨ 4 äº¿å¯¹ï¼ˆå›¾åƒ-æ–‡æœ¬ï¼‰æ•°æ®ä¸Šè®­ç»ƒï¼Œå®ç°å¼ºå¤§çš„ Zero-Shot è¿ç§»èƒ½åŠ›ã€‚åŒå¡”æ¶æ„ï¼ˆViT + Text Transformerï¼‰+ å¯¹ç§°äº¤å‰ç†µæŸå¤±ï¼Œæ— éœ€å¾®è°ƒç›´æ¥è¿ç§»åˆ° 30+ ä¸‹æ¸¸ä»»åŠ¡ï¼ŒZero-Shot ImageNet å‡†ç¡®ç‡è¾¾ 76.2%ã€‚',
        tags: ['CLIP', 'Contrastive Learning', 'Zero-Shot', 'OpenAI'],
        path: '/papers/clip'
      },
      {
        id: 'blip',
        title: 'BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation',
        authors: 'Salesforce Research (Li et al.)',
        date: '2022-01',
        description: 'ã€ç»Ÿä¸€ç†è§£ä¸ç”Ÿæˆã€‘æå‡º MED (Multimodal Mixture of Encoder-Decoder) æ¶æ„ï¼Œåœ¨å•ä¸€æ¨¡å‹ä¸­å®ç°å›¾æ–‡ç†è§£ä¸ç”Ÿæˆã€‚é€šè¿‡ CapFilt (Captioning and Filtering) æœºåˆ¶å‡€åŒ–ç½‘ç»œæ•°æ®ï¼Œæ˜¾è‘—æå‡å¤šæ¨¡æ€ä»»åŠ¡æ€§èƒ½ã€‚',
        tags: ['BLIP', 'Multimodal', 'Encoder-Decoder', 'Data Filtering'],
        path: '/papers/blip'
      },
      {
        id: 'blip2',
        title: 'BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models',
        authors: 'Salesforce Research (Li et al.)',
        date: '2023-01',
        description: 'ã€Q-Former æ¶æ„ã€‘æå‡ºè½»é‡çº§ Querying Transformer (Q-Former) ä½œä¸ºè§†è§‰-è¯­è¨€æ¡¥æ¢ã€‚ä¸¤é˜¶æ®µé¢„è®­ç»ƒï¼šå†»ç»“å›¾åƒç¼–ç å™¨å­¦ä¹ è¡¨å¾ â†’ å†»ç»“ LLM å­¦ä¹ ç”Ÿæˆã€‚å¯è®­ç»ƒå‚æ•°ä»… 188Mï¼ŒZero-shot VQAv2 è¾¾ 65.0%ï¼Œè¶…è¶Š Flamingo-80Bã€‚',
        tags: ['BLIP-2', 'Q-Former', 'Frozen LLM', 'CVPR 2023'],
        path: '/papers/blip2'
      },
      {
        id: 'instructblip',
        title: 'InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning',
        authors: 'Salesforce Research (Dai et al.)',
        date: '2023-05',
        description: 'ã€æŒ‡ä»¤æ„ŸçŸ¥è§†è§‰ç‰¹å¾ã€‘åœ¨ BLIP-2 åŸºç¡€ä¸Šå¼•å…¥æŒ‡ä»¤å¾®è°ƒã€‚æ ¸å¿ƒåˆ›æ–°ï¼šQ-Former åŒæ—¶æ¥æ”¶å›¾åƒå’ŒæŒ‡ä»¤ï¼ŒåŠ¨æ€æå–ä»»åŠ¡ç›¸å…³çš„è§†è§‰ç‰¹å¾ã€‚26 ä¸ªæ•°æ®é›†è®­ç»ƒï¼Œ13 ä¸ª Held-out ä»»åŠ¡å…¨éƒ¨ SOTAã€‚å¹³æ–¹æ ¹é‡‡æ ·ç­–ç•¥è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚',
        tags: ['InstructBLIP', 'Instruction Tuning', 'Zero-Shot', 'NeurIPS 2023'],
        path: '/papers/instructblip'
      },
      {
        id: 'siglip',
        title: 'SigLIP: Sigmoid Loss for Language Image Pre-Training',
        authors: 'Google DeepMind (Zhai et al.)',
        date: '2023-03',
        description: 'ã€Sigmoid å¯¹æ¯”å­¦ä¹ ã€‘ç”¨ Sigmoid æŸå¤±æ›¿ä»£ Softmaxï¼Œå°†å¯¹æ¯”å­¦ä¹ é—®é¢˜é‡æ–°å®šä¹‰ä¸ºæˆå¯¹äºŒåˆ†ç±»ã€‚å†…å­˜æ¶ˆè€—ä¸å†éš Batch Size å¹³æ–¹å¢é•¿ï¼Œ32k Batch å³è¾¾æ€§èƒ½é¥±å’Œç‚¹ã€‚4 ä¸ª TPUv4 èŠ¯ç‰‡ 2 å¤©è¾¾åˆ° 84.5% ImageNet 0-shotã€‚',
        tags: ['SigLIP', 'Sigmoid Loss', 'ICCV 2023', 'Efficient Training'],
        path: '/papers/siglip'
      },
      {
        id: 'eva',
        title: 'EVA: Exploring the Limits of Masked Visual Representation Learning at Scale',
        authors: 'BAAI (Fang et al.)',
        date: '2022-11',
        description: 'ã€10 äº¿å‚æ•° ViTã€‘å®Œå…¨åŸºäºå…¬å¼€æ•°æ®ï¼Œé€šè¿‡é¢„æµ‹è¢«æ©ç å›¾åƒåŒºåŸŸå¯¹åº”çš„ CLIP è§†è§‰ç‰¹å¾è¿›è¡Œé¢„è®­ç»ƒã€‚Vanilla ViT æ¶æ„ + è´Ÿä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±ã€‚ImageNet è¾¾ 89.7% Top-1ï¼ŒLVIS ä¸ COCO å®ä¾‹åˆ†å‰²å·®è·å‡ ä¹æ¶ˆé™¤ï¼Œå±•ç°å¼ºå¤§è¯­ä¹‰æ³›åŒ–èƒ½åŠ›ã€‚',
        tags: ['EVA', 'MIM', 'CLIP Feature', 'CVPR 2022'],
        path: '/papers/eva'
      },
      {
        id: 'eva-clip',
        title: 'EVA-CLIP: å¤§è§„æ¨¡ CLIP è®­ç»ƒæŠ€æœ¯çš„æ”¹è¿›',
        authors: 'BAAI (æ™ºæºç ”ç©¶é™¢)',
        date: '2023-03',
        description: 'ã€é«˜æ•ˆ CLIP è®­ç»ƒã€‘ç»“åˆ EVA åˆå§‹åŒ– + LAMB ä¼˜åŒ–å™¨ + Flash Attention + FLIP Token Droppingï¼Œ5.0B å‚æ•°æ¨¡å‹ä»…éœ€ 9B æ ·æœ¬å³è¾¾ 82.0% ImageNet Zero-shot Top-1ã€‚æ·±åº¦å¯¹æ¯” AdamW vs LAMB ä¼˜åŒ–å™¨åŸç†ï¼ŒCLIP InfoNCE æŸå¤±å‡½æ•°å…¬å¼è¯¦è§£ã€‚',
        tags: ['EVA-CLIP', 'LAMB', 'Flash Attention', 'CVPR 2023'],
        path: '/papers/eva-clip'
      }
    ]
  },
  {
    id: 'popular-mllm',
    title: 'Popular MLLMs å¸¸ç”¨å¤šæ¨¡æ€å¤§æ¨¡å‹',
    papers: [
      {
        id: 'flamingo',
        title: 'Flamingo: a Visual Language Model for Few-Shot Learning',
        authors: 'DeepMind',
        date: 'NeurIPS 2022',
        description: 'ã€Few-Shot VLM é‡Œç¨‹ç¢‘ã€‘é€šè¿‡ Perceiver Resampler å°†ä»»æ„é•¿åº¦è§†è§‰ç‰¹å¾å‹ç¼©ä¸ºå›ºå®š Tokenï¼Œé—¨æ§äº¤å‰æ³¨æ„åŠ› (tanh(Î±)=0 åˆå§‹åŒ–) ç¨³å®šèåˆå†»ç»“çš„ NFNet + Chinchilla LLMã€‚M3W äº¤é”™å›¾æ–‡æ•°æ®èµ‹äºˆå¼ºå¤§ In-context Learning èƒ½åŠ›ï¼Œ32-shot è¶…è¶Šå¤šä»»åŠ¡ Fine-tuned SOTAã€‚',
        tags: ['Flamingo', 'Few-Shot', 'Perceiver', 'Gated XATTN'],
        path: '/papers/flamingo'
      },
      {
        id: 'llava',
        title: 'LLaVA: Visual Instruction Tuning',
        authors: 'Microsoft & UW-Madison',
        date: 'NeurIPS 2023',
        description: 'ã€è§†è§‰æŒ‡ä»¤å¾®è°ƒå¼€åˆ›ä¹‹ä½œã€‘é¦–æ¬¡ä½¿ç”¨ GPT-4 ç”Ÿæˆ 158K å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®ã€‚æç®€æ¶æ„ï¼šLinear Projection è¿æ¥ CLIP ViT + Vicuna LLMï¼Œä¸¤é˜¶æ®µè®­ç»ƒï¼ˆç‰¹å¾å¯¹é½â†’ç«¯åˆ°ç«¯å¾®è°ƒï¼‰ã€‚ScienceQA SOTA (92.53%)ï¼Œå¼€æºæ¨åŠ¨ LLaVA-1.5/NeXT/OneVision ç­‰åç»­å‘å±•ã€‚',
        tags: ['LLaVA', 'Visual Instruction', 'GPT-4 Data', 'Open Source'],
        path: '/papers/llava'
      },
      {
        id: 'llava-1.5',
        title: 'LLaVA-1.5: Improved Baselines with Visual Instruction Tuning',
        authors: 'Microsoft & UW-Madison',
        date: 'CVPR 2024',
        description: 'ã€LLaVA ç³»ç»Ÿæ€§æ”¹è¿›ã€‘ä¸‰ä¸ªå…³é”®å‡çº§ï¼šMLP è¿æ¥å™¨æ›¿ä»£ Linear Projection + CLIP-ViT-L-336px é«˜åˆ†è¾¨ç‡ + å­¦æœ¯ä»»åŠ¡æ•°æ®ï¼ˆVQA/GQA/OCRVQAï¼‰ã€‚å“åº”æ ¼å¼æç¤ºå¹³è¡¡é•¿çŸ­å›ç­”ã€‚ä»… 1.2M å…¬å¼€æ•°æ®ï¼Œ1 å¤©è®­ç»ƒï¼Œ11 é¡¹åŸºå‡† SOTAã€‚',
        tags: ['LLaVA-1.5', 'MLP Connector', '336px', 'Data Efficient'],
        path: '/papers/llava-1.5'
      },
      {
        id: 'llava-next',
        title: 'LLaVA-NeXT Ablations: Beyond Data',
        authors: 'LLaVA-VL Team',
        date: '2024-05',
        description: 'ã€LLaVA-NeXT æ¶ˆèå®éªŒã€‘æ·±åº¦å‰–ææ¶æ„é€‰æ‹©ï¼šLLM è¶Šå¼ºå¤šæ¨¡æ€è¶Šå¼ºï¼ŒSigLIP-SO400M æœ€ä½³æ€§ä»·æ¯”ã€‚Higher-AnyRes + é˜ˆå€¼åŒçº¿æ€§æ’å€¼å…¬å¼æ”¯æŒ 6Ã—6 ç½‘æ ¼ã€‚Stage 1.5 é«˜è´¨é‡é¢„çƒ­è‡³å…³é‡è¦ï¼šå…¨é‡å¾®è°ƒ + Re-captioned åˆæˆæ•°æ®æ˜¾è‘—æå‡æ€§èƒ½ã€‚',
        tags: ['LLaVA-NeXT', 'Ablation', 'Higher-AnyRes', 'Stage 1.5'],
        path: '/papers/llava-next'
      },
      {
        id: 'deepseek-vl2',
        title: 'DeepSeek-VL2: Mixture-of-Experts Vision-Language Models',
        authors: 'DeepSeek-AI',
        date: '2024-12',
        description: 'ã€MoE è§†è§‰è¯­è¨€æ¨¡å‹ã€‘å¼•å…¥åŠ¨æ€åˆ†å—ï¼ˆDynamic Tilingï¼‰å¤„ç†ä»»æ„å®½é«˜æ¯”é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œé‡‡ç”¨ DeepSeekMoE + MLA æ¶æ„å®ç°é«˜æ•ˆæ¨ç†ã€‚ä¸‰ç‰ˆæœ¬ Tiny/Small/Standard (1.0B/2.8B/4.5B æ¿€æ´»å‚æ•°)ï¼Œåœ¨ OCRã€æ–‡æ¡£ç†è§£ã€è§†è§‰å®šä½ç­‰ä»»åŠ¡ä¸Šè¾¾åˆ° SOTAã€‚',
        tags: ['DeepSeek-VL2', 'MoE', 'Dynamic Tiling', 'MLA'],
        path: '/papers/deepseek-vl2'
      },
      {
        id: 'qwen-vl',
        title: 'Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond',
        authors: 'Alibaba Group',
        date: '2023-08',
        description: 'ã€é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ã€‘åŸºäº Qwen-7B çš„å¤šæ¨¡æ€æ‰©å±•ï¼Œé€šè¿‡ä½ç½®æ„ŸçŸ¥ Cross-Attention Adapter å°† ViT-bigG è§†è§‰ç‰¹å¾å‹ç¼©ä¸ºå›ºå®š 256 Tokenã€‚ä¸‰é˜¶æ®µè®­ç»ƒï¼š14 äº¿å›¾æ–‡å¯¹é¢„è®­ç»ƒâ†’å¤šä»»åŠ¡é¢„è®­ç»ƒâ†’æŒ‡ä»¤å¾®è°ƒã€‚æ”¯æŒå›¾åƒæè¿°ã€VQAã€OCR å’Œè§†è§‰å®šä½ (Grounding)ï¼Œ448Ã—448 é«˜åˆ†è¾¨ç‡è¾“å…¥ã€‚',
        tags: ['Qwen-VL', 'Cross-Attention', 'Grounding', 'OCR'],
        path: '/papers/qwen-vl'
      },
      {
        id: 'qwen2-vl',
        title: 'Qwen2-VL: Enhancing Vision-Language Model\'s Perception of the World at Any Resolution',
        authors: 'Alibaba Qwen Team',
        date: '2024-09',
        description: 'ã€åŠ¨æ€åˆ†è¾¨ç‡ VLMã€‘é‡æ–°å®šä¹‰è§†è§‰å¤„ç†èŒƒå¼ï¼šåŸç”ŸåŠ¨æ€åˆ†è¾¨ç‡ (Naive Dynamic Resolution) å¤„ç†ä»»æ„å°ºå¯¸å›¾åƒ + M-RoPE å¤šæ¨¡æ€æ—‹è½¬ä½ç½®ç¼–ç èåˆæ—¶é—´/é«˜åº¦/å®½åº¦ä¸‰ç»´ä¿¡æ¯ã€‚3D å·ç§¯ç»Ÿä¸€å›¾åƒè§†é¢‘ç†è§£ï¼Œ2Ã—2 æ± åŒ–å‹ç¼© Tokenã€‚72B ç‰ˆæœ¬åœ¨ DocVQAã€MathVista è¶…è¶Š GPT-4oã€‚',
        tags: ['Qwen2-VL', 'M-RoPE', 'Dynamic Resolution', 'Video'],
        path: '/papers/qwen2-vl'
      },
      {
        id: 'qwen2.5-vl',
        title: 'Qwen2.5-VL Technical Report',
        authors: 'Alibaba Qwen Team',
        date: '2025-02',
        description: 'ã€ç»†ç²’åº¦æ„ŸçŸ¥ + Agentã€‘Window Attention ä¼˜åŒ– ViT æ¨ç†æ•ˆç‡ + ç»å¯¹æ—¶é—´ MRoPE å®ç°ç§’çº§è§†é¢‘äº‹ä»¶å®šä½ + åŠ¨æ€ FPS é‡‡æ ·æ”¯æŒå°æ—¶çº§è§†é¢‘ã€‚4.1T é¢„è®­ç»ƒæ•°æ®ï¼ˆâ†‘3.4xï¼‰ã€‚å…¨èƒ½æ–‡æ¡£è§£æï¼ˆæ‰‹å†™/è¡¨æ ¼/å›¾è¡¨/åŒ–å­¦å…¬å¼/ä¹è°±ï¼‰+ ç²¾ç¡®å¯¹è±¡å®šä½ + ç”µè„‘/æ‰‹æœºæ“ä½œ Agent èƒ½åŠ›ã€‚',
        tags: ['Qwen2.5-VL', 'Window Attention', 'Absolute Time', 'Visual Agent'],
        path: '/papers/qwen2.5-vl'
      },
      {
        id: 'qwen2.5-omni',
        title: 'Qwen2.5-Omni Technical Report',
        authors: 'Qwen Team, Alibaba Group',
        date: '2025-03',
        description: 'ã€ç«¯åˆ°ç«¯å…¨æ¨¡æ€æµå¼äº¤äº’ã€‘Thinker-Talker åŒæ¨¡å—æ¶æ„è§£è€¦æ¨ç†ä¸è¯­éŸ³ç”Ÿæˆ + TMROPE å®ç°éŸ³ç”»ç²¾ç¡®åŒæ­¥ï¼ˆ40msç²¾åº¦ï¼‰+ æ»‘åŠ¨çª—å£ DiT å®ç°æ¯«ç§’çº§æµå¼è¾“å‡ºã€‚æ”¯æŒæ–‡æœ¬/å›¾åƒ/éŸ³é¢‘/è§†é¢‘ç»Ÿä¸€æ„ŸçŸ¥ä¸æ–‡æœ¬+è¯­éŸ³åŒæ¨¡æ€ç”Ÿæˆã€‚DPO ä¼˜åŒ–è¯­éŸ³è´¨é‡ï¼Œå‡»è´¥ä¸“é—¨ TTS æ¨¡å‹ã€‚',
        tags: ['Qwen2.5-Omni', 'Thinker-Talker', 'TMROPE', 'Streaming', 'End-to-End'],
        path: '/papers/qwen2.5-omni'
      },
      {
        id: 'janus',
        title: 'Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation',
        authors: 'DeepSeek-AI',
        date: '2024-10',
        description: 'ã€è§£è€¦è§†è§‰ç¼–ç çš„ç»Ÿä¸€æ¨¡å‹ã€‘ç†è§£ä»»åŠ¡ç”¨ SigLIP æå–é«˜å±‚è¯­ä¹‰ï¼Œç”Ÿæˆä»»åŠ¡ç”¨ VQ Tokenizer ä¿ç•™åƒç´ ç»†èŠ‚ï¼ŒåŒè·¯å¾„å…±äº«åŒä¸€ LLMã€‚1.3B å‚æ•°è¶…è¶Š LLaVA-1.5 7B (POPE 87.0 vs 85.9)ï¼ŒGenEval å‡»è´¥ SDXL å’Œ DALL-E 2ã€‚CFG å¼•å¯¼å¢å¼ºæ–‡å›¾ä¸€è‡´æ€§ã€‚',
        tags: ['Janus', 'Decoupling', 'SigLIP', 'VQ Tokenizer', 'CFG'],
        path: '/papers/janus'
      },
      {
        id: 'kimi-vl',
        title: 'Kimi-VL Technical Report',
        authors: 'Moonshot AI',
        date: '2025-04',
        description: 'ã€é«˜æ•ˆ MoE è§†è§‰è¯­è¨€æ¨¡å‹ã€‘16B æ€»å‚æ•°ä»… 2.8B æ¿€æ´»ï¼Œ128K é•¿ä¸Šä¸‹æ–‡ã€‚MoonViT åŸç”Ÿåˆ†è¾¨ç‡ + NaViT Packing + 2D RoPEã€‚Thinking-2506 ç‰ˆæœ¬ï¼šMathVista 80.1ã€MathVision 56.9ã€VideoMMMU 65.2ï¼ˆè¶… GPT-4oï¼‰ã€‚Long-CoT + RL è®­ç»ƒï¼ŒToken æ¶ˆè€—å‡å°‘ 20%ã€‚ScreenSpot-Pro 52.8 è¿œè¶… GPT-4o (0.8)ã€‚',
        tags: ['Kimi-VL', 'MoE', 'MoonViT', 'Long-CoT', '128K Context'],
        path: '/papers/kimi-vl'
      }
    ]
  },
  {
    id: 'iquest-jiukun-series',
    title: 'IQuest_ä¹å¤ Series',
    papers: [
      {
        id: 'iquest-coder-v1',
        title: 'IQuest-Coder-V1 Technical Report',
        authors: 'IQuest Coder Team',
        date: '2025',
        description: 'ã€Code-Flow å¤šé˜¶æ®µè®­ç»ƒèŒƒå¼ã€‘è¶…è¶Šé™æ€ä»£ç è¡¨ç¤ºï¼Œ7B/14B/40B å…¨ç³»åˆ— + LoopCoder å¾ªç¯æ¶æ„ã€‚æå‡ºå¤šè¯­è¨€ Scaling Law æ˜¾å¼å»ºæ¨¡è·¨è¯­è¨€è¿ç§»ã€‚SWE-Bench 81.4%ã€HumanEval+ 90.2ã€CRUXEval 98.9%ã€‚Thinking Path æ¿€å‘é•¿ç¨‹æ¨ç†ä¸é”™è¯¯è‡ªä¿®æ­£èƒ½åŠ›ã€‚',
        tags: ['IQuest-Coder', 'Code-Flow', 'LoopCoder', 'Scaling Law', 'SWE-Bench'],
        path: '/papers/iquest-coder-v1'
      }
    ]
  }
];

const PaperCard = ({ paper }) => (
  <Link 
    to={paper.path} 
    className="block bg-slate-900 border border-slate-800 rounded-xl p-6 hover:border-blue-500/50 hover:bg-slate-800/50 transition-all group h-full"
  >
    <div className="flex justify-between items-start mb-4">
      <div className="flex gap-2 flex-wrap">
        {paper.tags.map(tag => (
          <span key={tag} className="px-2 py-1 bg-blue-500/10 text-blue-400 text-xs rounded-md font-medium border border-blue-500/20">
            {tag}
          </span>
        ))}
      </div>
      <ArrowRight className="text-slate-500 group-hover:text-blue-400 transition-colors shrink-0" size={20} />
    </div>
    
    <h3 className="text-xl font-bold text-slate-100 mb-2 group-hover:text-blue-300 transition-colors">
      {paper.title}
    </h3>
    
    <div className="flex items-center gap-4 text-sm text-slate-400 mb-4">
      <span className="flex items-center gap-1">
        <BookOpen size={14} />
        {paper.authors}
      </span>
      <span className="flex items-center gap-1">
        <Calendar size={14} />
        {paper.date}
      </span>
    </div>
    
    <p className="text-slate-400 text-sm line-clamp-4">
      {paper.description}
    </p>
  </Link>
);

const Home = () => {
  const [activeSection, setActiveSection] = useState('');

  const scrollToSection = (id) => {
    const element = document.getElementById(id);
    if (element) {
      element.scrollIntoView({ behavior: 'smooth' });
      setActiveSection(id);
    }
  };

  useEffect(() => {
    const handleScroll = () => {
      const scrollPosition = window.scrollY + 150;
      
      for (const group of paperGroups) {
        const element = document.getElementById(group.id);
        if (element) {
          const { offsetTop, offsetHeight } = element;
          if (scrollPosition >= offsetTop && scrollPosition < offsetTop + offsetHeight) {
            setActiveSection(group.id);
            break;
          }
        }
      }
    };

    window.addEventListener('scroll', handleScroll);
    handleScroll(); // åˆå§‹åŒ–
    return () => window.removeEventListener('scroll', handleScroll);
  }, []);

  return (
    <div className="min-h-screen bg-slate-950 text-slate-300 font-sans selection:bg-blue-500/30">
      {/* å·¦ä¾§å›ºå®šå¯¼èˆªæ  */}
      <aside className="fixed left-0 top-0 h-full w-56 bg-slate-900/80 backdrop-blur-md border-r border-slate-800 z-40 hidden lg:flex flex-col">
        <div className="p-6 border-b border-slate-800">
          <div className="flex items-center gap-2 text-blue-400">
            <List size={20} />
            <span className="font-semibold text-sm">ç›®å½•å¯¼èˆª</span>
          </div>
        </div>
        <nav className="flex-1 overflow-y-auto py-4">
          {paperGroups.map((group) => (
            <button
              key={group.id}
              onClick={() => scrollToSection(group.id)}
              className={`w-full text-left px-6 py-3 text-sm transition-all flex items-center justify-between group ${
                activeSection === group.id
                  ? 'bg-blue-500/10 text-blue-400 border-r-2 border-blue-500'
                  : 'text-slate-400 hover:bg-slate-800/50 hover:text-slate-200'
              }`}
            >
              <span className="truncate">{group.title}</span>
              <span className={`text-xs px-1.5 py-0.5 rounded ${
                activeSection === group.id
                  ? 'bg-blue-500/20 text-blue-300'
                  : 'bg-slate-800 text-slate-500 group-hover:bg-slate-700'
              }`}>
                {group.papers.length}
              </span>
            </button>
          ))}
        </nav>
        <div className="p-4 border-t border-slate-800 text-xs text-slate-600">
          å…± {paperGroups.reduce((sum, g) => sum + g.papers.length, 0)} ç¯‡è®ºæ–‡
        </div>
      </aside>

      {/* ä¸»å†…å®¹åŒºåŸŸ */}
      <div className="lg:ml-56">
        <header className="pt-24 pb-12 px-6">
          <div className="container mx-auto max-w-5xl">
            <h1 className="text-4xl font-bold text-white mb-4">
              Paper Reading <span className="text-blue-500">Log</span>
            </h1>
            <p className="text-slate-400 text-lg max-w-2xl">
              Personal collection of paper notes, technical deep dives, and architectural breakdowns.
            </p>
          </div>
        </header>

        <main className="container mx-auto px-6 max-w-5xl pb-24 space-y-12">
          {paperGroups.map((group) => (
            <section key={group.id} id={group.id} className="scroll-mt-8 animate-fade-in">
              <div className="flex items-center gap-3 mb-6 border-b border-slate-800 pb-4">
                <div className="p-2 bg-blue-500/10 rounded-lg text-blue-400">
                  <FolderOpen size={20} />
                </div>
                <h2 className="text-2xl font-bold text-slate-100">
                  {group.title}
                </h2>
                <span className="px-2 py-0.5 bg-slate-800 text-slate-400 text-xs rounded-full">
                  {group.papers.length}
                </span>
              </div>
              
              {group.papers.length > 0 ? (
                <div className="grid md:grid-cols-2 gap-6">
                  {group.papers.map(paper => (
                    <PaperCard key={paper.id} paper={paper} />
                  ))}
                </div>
              ) : (
                <div className="text-slate-500 text-sm italic p-4 border border-slate-800 border-dashed rounded-xl text-center">
                  No papers added yet. Coming soon...
                </div>
              )}
            </section>
          ))}
        </main>

        <footer className="border-t border-slate-900 py-8 text-center text-slate-600 text-sm">
          <div className="container mx-auto">
            Â© 2025 Paper Reading Log
          </div>
        </footer>
      </div>
    </div>
  );
};

export default Home;
